import os
import re
import json
import time
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv
import google.generativeai as genai

# =========================
# Configura√ß√£o
# =========================
load_dotenv()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if not GEMINI_API_KEY:
    raise ValueError("GEMINI_API_KEY n√£o encontrada no arquivo .env")

genai.configure(api_key=GEMINI_API_KEY)

# Usando gemini-2.5-flash: r√°pido, eficiente e com √≥tima qualidade
MODEL_STRUCT = "gemini-2.5-flash"
MODEL_EVAL = "gemini-2.5-flash"

# Temperaturas: baixa para previsibilidade e JSON consistente
GEN_CONFIG_JSON = {
    "temperature": 0.2,
    "response_mime_type": "application/json",
}


# =========================
# Helpers Gerais
# =========================
def _extract_json(text: str) -> Any:
    """Extrai JSON de uma string. Tenta json.loads direto; se falhar, usa regex no primeiro bloco {...}."""
    text = text.strip()
    # remove cercas de c√≥digo se existirem
    if text.startswith("```json"):
        text = text[7:]
    if text.startswith("```"):
        text = text[3:]
    if text.endswith("```"):
        text = text[:-3]
    try:
        return json.loads(text)
    except Exception:
        pass

    # fallback: pega o primeiro bloco {...} balanceado simples
    match = re.search(r'\{.*\}', text, flags=re.DOTALL)
    if match:
        candidate = match.group(0)
        try:
            return json.loads(candidate)
        except Exception:
            pass
    raise ValueError("Falha ao extrair JSON do retorno do modelo.")


def _gemini_generate_json(
    model_name: str,
    system_instruction: Optional[str],
    user_prompt: str,
    max_retries: int = 3,
    backoff_sec: float = 1.2,
) -> Any:
    """
    Faz uma chamada ao Gemini esperando JSON e retorna objeto Python.
    For√ßa MIME de resposta em JSON e aplica retries com backoff exponencial.
    """
    model = genai.GenerativeModel(model_name, system_instruction=system_instruction) if system_instruction \
        else genai.GenerativeModel(model_name)

    last_err = None
    for attempt in range(1, max_retries + 1):
        try:
            resp = model.generate_content(
                user_prompt,
                generation_config=GEN_CONFIG_JSON,
            )
            # Em SDKs recentes, JSON vem em resp.text mesmo com mime JSON.
            return _extract_json(resp.text)
        except Exception as e:
            last_err = e
            if attempt < max_retries:
                time.sleep(backoff_sec * attempt)
            else:
                raise last_err


def _round_to_step(value: float, step: float, max_score: float) -> float:
    """Arredonda a 'step' e limita ao intervalo [0, max_score]."""
    if value is None:
        value = 0.0
    rounded = round(float(value) / step) * step
    # pequena corre√ß√£o de flutua√ß√£o
    rounded = float(f"{rounded:.2f}")
    if rounded < 0:
        rounded = 0.0
    if rounded > max_score:
        rounded = float(max_score)
    return rounded


# =========================
# 1) Parsing do OCR
# =========================
def parse_ocr_text(ocr_text: str) -> str:
    """
    Usa a LLM para limpar o texto OCR de ru√≠dos e normaliza√ß√µes.
    Remove cabe√ßalhos, rodap√©s, n√∫meros de p√°gina e outros elementos n√£o relevantes.
    Corrige hifeniza√ß√£o e problemas de OCR, mas mant√©m a estrutura original do conte√∫do.
    
    Args:
        ocr_text: Texto bruto extra√≠do do OCR
        
    Returns:
        Texto limpo como string √∫nica (n√£o segmentado)
    """
    if not ocr_text or not ocr_text.strip():
        return ""
    
    # Pr√©-processamento b√°sico (universal e r√°pido)
    text = ocr_text.strip()
    
    # Normaliza hifeniza√ß√£o √≥bvia
    text = re.sub(r'(\w)-\s*\n\s*(\w)', r'\1\2', text)
    
    # Compacta espa√ßos excessivos
    text = re.sub(r'[ \t]+', ' ', text)
    text = re.sub(r'\n{4,}', '\n\n\n', text)  # Limita a 3 quebras
    
    # Usa LLM para limpeza inteligente
    system_instruction = (
        "Voc√™ √© um assistente especializado em limpeza de texto OCR de provas escolares. "
        "Seu trabalho √© remover ru√≠dos mantendo TODO o conte√∫do das quest√µes e respostas."
    )
    
    user_prompt = {
        "tarefa": "Limpar texto OCR mantendo quest√µes e respostas intactas",
        "instrucoes": [
            "MANTENHA todo o conte√∫do de quest√µes e respostas dos alunos.",
            "REMOVA: n√∫meros de p√°gina, cabe√ßalhos (Nome, Data, Turma, Matr√≠cula), rodap√©s, marcas d'√°gua.",
            "CORRIJA: palavras quebradas por h√≠fen, caracteres OCR mal interpretados, espa√ßamentos estranhos.",
            "PRESERVE: a estrutura original, numera√ß√£o de quest√µes, formata√ß√£o de respostas.",
            "N√ÉO adicione ou modifique conte√∫do - apenas limpe ru√≠dos t√©cnicos."
        ],
        "texto_ocr": text,
        "formato_saida": {
            "texto_limpo": "string com o texto processado"
        }
    }
    
    try:
        result = _gemini_generate_json(
            model_name=MODEL_STRUCT,
            system_instruction=system_instruction,
            user_prompt=json.dumps(user_prompt, ensure_ascii=False),
        )
        
        cleaned_text = result.get("texto_limpo", "").strip()
        
        # Fallback: se o retorno estiver vazio ou muito pequeno, retorna o original pr√©-processado
        if not cleaned_text or len(cleaned_text) < len(text) * 0.3:
            return text
            
        return cleaned_text
        
    except Exception as e:
        print(f"Aviso: Erro na limpeza com LLM. Usando texto pr√©-processado. Erro: {e}")
        return text


# =========================
# 2) Estrutura√ß√£o (OCR -> JSON)
# =========================
def _gerar_id_questao(questao: Dict[str, Any]) -> str:
    """
    Gera um ID √∫nico para a quest√£o usando n√∫mero + letra (se existir).
    Exemplo: "1-a", "1-b", "2" (se n√£o tiver letra)
    """
    numero = questao.get("numero", "")
    letra = questao.get("letra", "").lower().strip()
    
    if letra:
        return f"{numero}-{letra}"
    return str(numero)


def structure_exam_json(cleaned_text: str, registered_questions: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Usa LLM para identificar, segmentar e associar respostas do OCR √†s perguntas discursivas.
    A LLM faz TODO o trabalho de estrutura√ß√£o: identifica√ß√£o de quest√µes, segmenta√ß√£o e associa√ß√£o.
    Retorna somente quest√µes discursivas.
    
    Args:
        cleaned_text: Texto limpo do OCR (string √∫nica)
        registered_questions: Lista com metadados das quest√µes da prova
        
    Returns:
        JSON estruturado com quest√µes discursivas e respostas dos alunos
    """
    discursive_questions = [q for q in registered_questions if q.get("tipo", "").lower() == "discursiva"]
    if not discursive_questions:
        return {"questoes": []}

    # Cria mapa de quest√µes por ID √∫nico para facilitar busca
    questoes_map = {_gerar_id_questao(q): q for q in discursive_questions}

    system_instruction = (
        "Voc√™ √© um agente de estrutura√ß√£o de respostas de prova. "
        "Voc√™ sempre retorna JSON v√°lido e nada al√©m de JSON."
    )

    user_prompt = {
        "tarefa": "Identificar, segmentar e associar respostas do aluno √†s quest√µes discursivas registradas.",
        "instrucoes": [
            "IDENTIFIQUE as quest√µes no texto OCR (podem ter v√°rios formatos: 'Quest√£o 1', '1)', 'a)', '1-a)', etc).",
            "ATEN√á√ÉO: Se a quest√£o tiver n√∫mero E letra (ex: '1-a', '1-b'), use AMBOS para identificar corretamente.",
            "SEGMENTE as respostas de cada quest√£o corretamente.",
            "ASSOCIE cada resposta √† quest√£o correspondente usando n√∫mero E letra (se existir).",
            "MANTENHA o texto original do aluno; corrija apenas erros t√©cnicos de OCR.",
            "Se n√£o encontrar resposta para alguma quest√£o registrada, use string vazia em 'resposta_aluno'.",
            "IGNORE quest√µes objetivas (m√∫ltipla escolha, verdadeiro/falso).",
            "Retorne APENAS JSON no formato especificado."
        ],
        "formato_saida": {
            "questoes": [
                {
                    "numero": "int - n√∫mero da quest√£o",
                    "letra": "string opcional - letra da alternativa (a, b, c, etc)",
                    "tipo": "discursiva",
                    "pergunta": "string",
                    "nota_maxima": "float",
                    "resposta_aluno": "string"
                }
            ]
        },
        "questoes_registradas_discursivas": discursive_questions,
        "texto_ocr_limpo": cleaned_text
    }

    try:
        result = _gemini_generate_json(
            model_name=MODEL_STRUCT,
            system_instruction=system_instruction,
            user_prompt=json.dumps(user_prompt, ensure_ascii=False),
        )
        # Valida√ß√£o m√≠nima
        if not isinstance(result, dict) or "questoes" not in result:
            raise ValueError("JSON inv√°lido: campo 'questoes' ausente.")
        
        questoes_estruturadas = []
        for q in result["questoes"]:
            # Gera ID da quest√£o retornada pela LLM
            q_id = _gerar_id_questao(q)
            
            # Busca a quest√£o registrada correspondente
            rq = questoes_map.get(q_id)
            
            if rq:
                # Usa os metadados originais como fonte de verdade
                questao_estruturada = {
                    "numero": rq["numero"],
                    "letra": rq.get("letra", ""),
                    "tipo": "discursiva",
                    "pergunta": rq["pergunta"],
                    "nota_maxima": rq["nota_maxima"],
                    "resposta_aluno": q.get("resposta_aluno", "").strip() or ""
                }
                questoes_estruturadas.append(questao_estruturada)
            else:
                # Se n√£o encontrou correspond√™ncia, tenta buscar apenas por n√∫mero
                # (para compatibilidade com quest√µes sem letra)
                numero = q.get("numero")
                rq_por_numero = next((x for x in discursive_questions if x["numero"] == numero and not x.get("letra")), None)
                if rq_por_numero:
                    questao_estruturada = {
                        "numero": rq_por_numero["numero"],
                        "letra": "",
                        "tipo": "discursiva",
                        "pergunta": rq_por_numero["pergunta"],
                        "nota_maxima": rq_por_numero["nota_maxima"],
                        "resposta_aluno": q.get("resposta_aluno", "").strip() or ""
                    }
                    questoes_estruturadas.append(questao_estruturada)
        
        # Garante que todas as quest√µes discursivas estejam presentes
        questoes_ids_encontradas = {_gerar_id_questao(q) for q in questoes_estruturadas}
        for q in discursive_questions:
            q_id = _gerar_id_questao(q)
            if q_id not in questoes_ids_encontradas:
                questoes_estruturadas.append({
                    "numero": q["numero"],
                    "letra": q.get("letra", ""),
                    "tipo": "discursiva",
                    "pergunta": q["pergunta"],
                    "nota_maxima": q["nota_maxima"],
                    "resposta_aluno": ""
                })
        
        return {"questoes": questoes_estruturadas}

    except Exception as e:
        print(f"Erro na estrutura√ß√£o: {e}")
        # Fallback coerente: devolve esqueleto com respostas vazias
        return {
            "questoes": [
                {
                    "numero": q["numero"],
                    "letra": q.get("letra", ""),
                    "tipo": "discursiva",
                    "pergunta": q["pergunta"],
                    "nota_maxima": q["nota_maxima"],
                    "resposta_aluno": ""
                }
                for q in discursive_questions
            ]
        }


# =========================
# 3) Corre√ß√£o (JSON -> notas + an√°lise)
# =========================
def evaluate_answer(
    student_answer: str,
    expected_answer: str,
    max_score: float,
    question_text: str = "",
    step: float = 0.5,
) -> Dict[str, Any]:
    """
    Avalia a resposta do aluno em rela√ß√£o ao gabarito.
    - Retorna nota em passos (ex.: 0.5) e an√°lise curta.
    """
    if not student_answer or not student_answer.strip():
        return {"nota": 0.0, "analise": "Resposta em branco ou n√£o identificada."}

    system_instruction = (
        "Voc√™ √© um professor corrigindo respostas discursivas. "
        "Retorne APENAS JSON v√°lido."
    )
    user_prompt = {
        "pergunta": question_text or "N√£o especificada",
        "resposta_esperada": expected_answer,
        "resposta_aluno": student_answer,
        "nota_maxima": max_score,
        "criterios": [
            "Corre√ß√£o conceitual",
            "Completude",
            "Coer√™ncia e clareza"
        ],
        "tarefa": (
            f"Atribua uma nota de 0 a {max_score} e uma an√°lise breve (1-2 frases). "
            "A an√°lise deve justificar objetivamente a pontua√ß√£o."
        ),
        "formato_saida": {"nota": "float", "analise": "string"}
    }

    try:
        result = _gemini_generate_json(
            model_name=MODEL_EVAL,
            system_instruction=system_instruction,
            user_prompt=json.dumps(user_prompt, ensure_ascii=False),
        )
        # Sanitiza√ß√£o da nota
        raw_score = float(result.get("nota", 0.0))
        score = _round_to_step(raw_score, step=step, max_score=max_score)
        analysis = (result.get("analise") or "").strip()
        if not analysis:
            analysis = "Avalia√ß√£o autom√°tica sem justificativa detalhada."
        return {"nota": score, "analise": analysis}

    except Exception as e:
        return {"nota": 0.0, "analise": f"Erro na avalia√ß√£o autom√°tica: {str(e)}"}


def evaluate_exam(structured_exam: Dict[str, Any], answer_key: List[Dict[str, Any]], step: float = 0.5) -> Dict[str, Any]:
    """
    Itera sobre as quest√µes discursivas e aplica evaluate_answer em cada uma.
    Retorna quest√µes corrigidas e a nota total.
    Usa ID √∫nico (n√∫mero + letra) para garantir alinhamento correto.
    """
    questoes_corrigidas = []
    nota_total = 0.0

    # Mapa do gabarito por ID √∫nico (n√∫mero + letra)
    answer_map = {_gerar_id_questao(item): item for item in answer_key}

    for q in structured_exam.get("questoes", []):
        if str(q.get("tipo", "")).lower() != "discursiva":
            continue

        # Gera ID √∫nico da quest√£o
        q_id = _gerar_id_questao(q)
        numero = q["numero"]
        letra = q.get("letra", "")
        student_answer = q.get("resposta_aluno", "") or ""
        max_score = float(q.get("nota_maxima", 0.0))
        question_text = q.get("pergunta", "")

        # Busca gabarito usando ID √∫nico
        gabarito_item = answer_map.get(q_id)
        
        # Fallback: tenta buscar apenas por n√∫mero (compatibilidade)
        if not gabarito_item:
            gabarito_item = next((item for item in answer_key if item.get("numero") == numero and not item.get("letra")), None)
        
        if not gabarito_item:
            questoes_corrigidas.append({
                "numero": numero,
                "letra": letra,
                "nota": 0.0,
                "analise": f"Gabarito n√£o encontrado para quest√£o {numero}{'-' + letra if letra else ''}."
            })
            continue

        esperado = gabarito_item.get("resposta_esperada", "")
        if not esperado:
            questoes_corrigidas.append({
                "numero": numero,
                "letra": letra,
                "nota": 0.0,
                "analise": f"Resposta esperada n√£o encontrada no gabarito para quest√£o {numero}{'-' + letra if letra else ''}."
            })
            continue

        aval = evaluate_answer(
            student_answer=student_answer,
            expected_answer=esperado,
            max_score=max_score,
            question_text=question_text,
            step=step,
        )

        questoes_corrigidas.append({
            "numero": numero,
            "letra": letra,
            "nota": aval["nota"],
            "analise": aval["analise"]
        })
        nota_total += float(aval["nota"])

    return {
        "questoes_corrigidas": questoes_corrigidas,
        "nota_total": float(f"{nota_total:.2f}")
    }


# =========================
# Exemplo de uso manual
# =========================
if __name__ == "__main__":
    print("‚úÖ M√≥dulo gemini.py carregado com sucesso!")
    print(f"üîë API Key configurada: {'Sim' if GEMINI_API_KEY else 'N√£o'}")
    print(f"üì¶ Modelos: {MODEL_STRUCT} (estrutura√ß√£o), {MODEL_EVAL} (avalia√ß√£o)")
    print("\nüí° Para exemplos de uso, execute: python exemplo_uso_gemini.py")
